{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, confusion_matrix, classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team_size</th>\n",
       "      <th>team_A_avg_mmr</th>\n",
       "      <th>team_B_avg_mmr</th>\n",
       "      <th>team_A_civs</th>\n",
       "      <th>team_B_civs</th>\n",
       "      <th>map_name</th>\n",
       "      <th>team_A_won</th>\n",
       "      <th>civ_A_vs_B_winrate</th>\n",
       "      <th>civ_A_vs_B_certainty</th>\n",
       "      <th>mmr_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2294.0</td>\n",
       "      <td>2297.0</td>\n",
       "      <td>french</td>\n",
       "      <td>house_of_lancaster</td>\n",
       "      <td>Dry Arabia</td>\n",
       "      <td>0</td>\n",
       "      <td>42.993025</td>\n",
       "      <td>0.503591</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2290.0</td>\n",
       "      <td>2165.0</td>\n",
       "      <td>mongols</td>\n",
       "      <td>chinese</td>\n",
       "      <td>Gorge</td>\n",
       "      <td>1</td>\n",
       "      <td>53.023663</td>\n",
       "      <td>0.112949</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2282.0</td>\n",
       "      <td>2173.0</td>\n",
       "      <td>japanese</td>\n",
       "      <td>chinese</td>\n",
       "      <td>Carmel</td>\n",
       "      <td>1</td>\n",
       "      <td>59.821429</td>\n",
       "      <td>0.135038</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>2279.0</td>\n",
       "      <td>ottomans</td>\n",
       "      <td>japanese</td>\n",
       "      <td>Lipany</td>\n",
       "      <td>0</td>\n",
       "      <td>44.265081</td>\n",
       "      <td>0.116866</td>\n",
       "      <td>-321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>2279.0</td>\n",
       "      <td>holy_roman_empire</td>\n",
       "      <td>english</td>\n",
       "      <td>Dry Arabia</td>\n",
       "      <td>0</td>\n",
       "      <td>51.759944</td>\n",
       "      <td>0.607073</td>\n",
       "      <td>-647.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   team_size  team_A_avg_mmr  team_B_avg_mmr        team_A_civs  \\\n",
       "0          1          2294.0          2297.0             french   \n",
       "1          1          2290.0          2165.0            mongols   \n",
       "2          1          2282.0          2173.0           japanese   \n",
       "3          1          1958.0          2279.0           ottomans   \n",
       "4          1          1632.0          2279.0  holy_roman_empire   \n",
       "\n",
       "          team_B_civs    map_name  team_A_won  civ_A_vs_B_winrate  \\\n",
       "0  house_of_lancaster  Dry Arabia           0           42.993025   \n",
       "1             chinese       Gorge           1           53.023663   \n",
       "2             chinese      Carmel           1           59.821429   \n",
       "3            japanese      Lipany           0           44.265081   \n",
       "4             english  Dry Arabia           0           51.759944   \n",
       "\n",
       "   civ_A_vs_B_certainty  mmr_gap  \n",
       "0              0.503591     -3.0  \n",
       "1              0.112949    125.0  \n",
       "2              0.135038    109.0  \n",
       "3              0.116866   -321.0  \n",
       "4              0.607073   -647.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"final2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"team_A_won\"])\n",
    "y = df[\"team_A_won\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "categorical_cols = ['team_A_civs', 'team_B_civs', 'map_name']\n",
    "\n",
    "# Define numerical columns (all others except target)\n",
    "numerical_cols = [col for col in X_train.columns if col not in categorical_cols]\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Predictor - Logistic Regression**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test,\n",
    "                       categorical_features, numeric_features):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a model using a pipeline. Times the process.\n",
    "\n",
    "    Returns: dict with accuracy, f1, log_loss, runtime\n",
    "    \"\"\"\n",
    "    # Preprocessor: scale numerics, one-hot encode categoricals\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "    # Full pipeline: preprocessing + model\n",
    "    pipe = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_prob = pipe.predict_proba(X_test)[:, 1]  # For log_loss\n",
    "    runtime = time.time() - start_time\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    loss = log_loss(y_test, y_prob)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'log_loss': loss,\n",
    "        'runtime': runtime,\n",
    "        'model': model.__class__.__name__\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second model: XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_xgboost(X_train, X_test, y_train, y_test):\n",
    "    start_time = time.time()\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', xgb.XGBClassifier(\n",
    "            n_estimators=1500,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=4,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    preds = pipeline.predict(X_test)\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    report = classification_report(y_test, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        \"confusion_matrix\":cm,\n",
    "        'runtime': runtime,\n",
    "        'model': \"XGBoost\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third Model: Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_nn(X_train_raw, X_test_raw, y_train, y_test,\n",
    "                          categorical_features, numeric_features,\n",
    "                          input_dim=None, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a PyTorch neural network model.\n",
    "    Returns: dict with accuracy, f1, log_loss, runtime\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    X_train = preprocessor.fit_transform(X_train_raw)\n",
    "    X_test = preprocessor.transform(X_test_raw)\n",
    "\n",
    "    # ==== PyTorch Dataset ====\n",
    "    class AoeDataset(Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            self.X = torch.tensor(X.toarray() if hasattr(X, \"toarray\") else X, dtype=torch.float32)\n",
    "            self.y = torch.tensor(y.to_numpy().reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "        def __len__(self): return len(self.y)\n",
    "        def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "    train_dataset = AoeDataset(X_train, y_train)\n",
    "    test_dataset = AoeDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # ==== Model ====\n",
    "    class AoeNet(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(input_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(128),  # helps with training stability\n",
    "\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(32, 1),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        def forward(self, x): return self.net(x)\n",
    "\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "            nn.init.zeros_(m.bias)\n",
    "        \n",
    "    model = AoeNet(input_dim or X_train.shape[1])\n",
    "    model.apply(init_weights)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # ==== Training ====\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            preds = model(X_batch)\n",
    "            loss = loss_fn(preds, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    # ==== Evaluation ====\n",
    "    model.eval()\n",
    "    y_probs, y_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            y_probs.extend(outputs.squeeze().numpy())\n",
    "            y_true.extend(y_batch.squeeze().numpy())\n",
    "\n",
    "    y_preds = [1 if p >= 0.5 else 0 for p in y_probs]\n",
    "\n",
    "    # ==== Metrics ====\n",
    "    acc = accuracy_score(y_true, y_preds)\n",
    "    f1 = f1_score(y_true, y_preds)\n",
    "    loss = log_loss(y_true, y_probs)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'log_loss': loss,\n",
    "        'runtime': runtime,\n",
    "        'model': \"PyTorchNN\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7515644555694618, 'f1_score': 0.6826538768984812, 'log_loss': 0.4921482479313774, 'runtime': 0.062001705169677734, 'model': 'LogisticRegression'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baran\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:32:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9974968710888611, 'confusion_matrix': array([[897,   0],\n",
      "       [  4, 697]], dtype=int64), 'runtime': 1.2619960308074951, 'model': 'XGBoost'}\n",
      "{'accuracy': 0.9036295369211514, 'f1_score': 0.8884057971014493, 'log_loss': 0.22601403286253133, 'runtime': 23.34657645225525, 'model': 'PyTorchNN'}\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', max_iter=50)\n",
    "\n",
    "log_results = train_and_evaluate(logreg, X_train, X_test, y_train, y_test,\n",
    "                             categorical_cols, numerical_cols)\n",
    "print(log_results)\n",
    "    \n",
    "xgb_results = train_evaluate_xgboost(X_train, X_test, y_train, y_test)\n",
    "print(xgb_results)\n",
    "\n",
    "nn_results = train_and_evaluate_nn(\n",
    "    X_train_raw=X_train,\n",
    "    X_test_raw=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    categorical_features=categorical_cols,\n",
    "    numeric_features=numerical_cols\n",
    ")\n",
    "\n",
    "print(nn_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we only worked with binary classifiers, now we need to adapt the models so that they can also predict a probability and not just the winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_xgboost_calibrated(X_train, X_test, y_train, y_test, preprocessor, plot_curve=True):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 🧹 Preprocess (fit only on train)\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "    # 📦 Base XGBoost model with conservative settings\n",
    "    base_model = xgb.XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 🔄 Wrap in calibrated classifier (sigmoid is smoother)\n",
    "    calibrated = CalibratedClassifierCV(estimator=base_model, method='sigmoid', cv=5)\n",
    "    calibrated.fit(X_train_transformed, y_train)\n",
    "\n",
    "    # 🔮 Predictions\n",
    "    y_prob = calibrated.predict_proba(X_test_transformed)[:, 1]\n",
    "    y_pred = calibrated.predict(X_test_transformed)\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    # 📊 Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    loss = log_loss(y_test, y_prob)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # 📈 Calibration curve\n",
    "    if plot_curve:\n",
    "        prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.plot(prob_pred, prob_true, marker='o', label='XGBoost (Calibrated)')\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')\n",
    "        plt.title(\"Calibration Curve\")\n",
    "        plt.xlabel(\"Predicted Probability\")\n",
    "        plt.ylabel(\"Empirical Probability\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # 📉 Confidence Score: avg distance from 0.5\n",
    "    avg_confidence = float(np.mean(np.abs(y_prob - 0.5)))  # Range: 0 (totally uncertain) to 0.5 (max confident)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'log_loss': loss,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report,\n",
    "        'runtime': runtime,\n",
    "        'average_confidence': avg_confidence,\n",
    "        'model': \"XGBoost (Calibrated)\",\n",
    "        'y_pred': y_pred,\n",
    "        'y_prob': y_prob,\n",
    "        'y_true': y_test\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
